<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Sharon Voon">
<meta name="dcterms.date" content="2024-08-28">
<meta name="description" content="An end-to-end data pipeline for instance segmentation and classification">

<title>Sharon Voon - Robotic Arm Vision Recognition</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../imgs/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../blogs/robotic_ams_vision_recognition/index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../imgs/favicon.png" alt="SV" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../blogs/robotic_ams_vision_recognition/index.html">
    <span class="navbar-title">Sharon Voon</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../#background" rel="" target="">
 <span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blogs.html" rel="" target="">
 <span class="menu-text">Blogs</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools tools-wide">
    <a href="https://github.com/s-voon" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
    <a href="https://www.linkedin.com/in/sharonvoon/" rel="" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-linkedin"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Robotic Arm Vision Recognition</h1>
                  <div>
        <div class="description">
          An end-to-end data pipeline for instance segmentation and classification
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ML</div>
                <div class="quarto-category">Data Pipeline</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author"><a href="https://s-voon.github.io/">Sharon Voon</a> </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Master of Data Science Student, UBC, 2024
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 28, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#problem-and-motivation" id="toc-problem-and-motivation" class="nav-link active" data-scroll-target="#problem-and-motivation">Problem and Motivation</a></li>
  <li><a href="#project-impact-and-real-world-applications" id="toc-project-impact-and-real-world-applications" class="nav-link" data-scroll-target="#project-impact-and-real-world-applications">Project Impact and Real-World Applications</a></li>
  <li><a href="#background-and-acknowledgement" id="toc-background-and-acknowledgement" class="nav-link" data-scroll-target="#background-and-acknowledgement">Background and Acknowledgement</a></li>
  <li><a href="#theory-and-approach" id="toc-theory-and-approach" class="nav-link" data-scroll-target="#theory-and-approach">Theory and Approach</a>
  <ul class="collapse">
  <li><a href="#mask-r-cnn-precision-in-object-detection-and-segmentation" id="toc-mask-r-cnn-precision-in-object-detection-and-segmentation" class="nav-link" data-scroll-target="#mask-r-cnn-precision-in-object-detection-and-segmentation">Mask R-CNN: Precision in Object Detection and Segmentation</a></li>
  <li><a href="#clipseg-sam-leveraging-advanced-segmentation-models" id="toc-clipseg-sam-leveraging-advanced-segmentation-models" class="nav-link" data-scroll-target="#clipseg-sam-leveraging-advanced-segmentation-models">CLIPSeg + SAM: Leveraging Advanced Segmentation Models</a></li>
  <li><a href="#pixel-ratio-conversion-translating-pixels-to-real-world-dimensions" id="toc-pixel-ratio-conversion-translating-pixels-to-real-world-dimensions" class="nav-link" data-scroll-target="#pixel-ratio-conversion-translating-pixels-to-real-world-dimensions">Pixel Ratio Conversion: Translating Pixels to Real-World Dimensions</a></li>
  </ul></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#a-comparative-analysis-mrcnn-vs-clipseg-sam" id="toc-a-comparative-analysis-mrcnn-vs-clipseg-sam" class="nav-link" data-scroll-target="#a-comparative-analysis-mrcnn-vs-clipseg-sam">A Comparative Analysis: MRCNN vs CLIPSeg + SAM</a></li>
  <li><a href="#challenges-and-learnings" id="toc-challenges-and-learnings" class="nav-link" data-scroll-target="#challenges-and-learnings">Challenges and Learnings</a></li>
  <li><a href="#future-work-and-opportunities-for-improvement" id="toc-future-work-and-opportunities-for-improvement" class="nav-link" data-scroll-target="#future-work-and-opportunities-for-improvement">Future Work and Opportunities for Improvement</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<style>
h1.title {
    color: white;
}
</style>
<p>Ever wondered how a driverless car like Tesla navigates the world so seamlessly? The secret lies in computer vision—a technology that enables these cars to “see” their surroundings, identifying objects and understanding their exact locations. But this isn’t just limited to autonomous vehicles. Computer vision is transforming industries, making everyday tasks more efficient and precise.</p>
<p>In partnership with <a href="https://analytika.ca/">Analytika</a>, my recent capstone project, “Robotic Arm Vision Recognition,” tapped into the incredible potential of computer vision to revolutionize robotic systems. By harnessing the power of cutting-edge models like Mask R-CNN, Segment Anything Model (SAM), and ClipSeg, we pushed the boundaries of what robotic arms can achieve in a real-world factory setting.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="cv_cover.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Robotic Arm in a bottle assembly factory</figcaption>
</figure>
</div>
<section id="problem-and-motivation" class="level2">
<h2 class="anchored" data-anchor-id="problem-and-motivation">Problem and Motivation</h2>
<p>In modern manufacturing, especially on bottle production lines, efficiency and precision are paramount. The challenge? Traditional methods often fell short in accuracy, leading to increased waste and downtime. Typically, each assembly line is tailored to a specific bottle shape and size, requiring manual adjustments during turnover to accommodate different types. Our mission was clear: to develop an advanced vision recognition system that could handle this variability with precision and automation, thus improving the production lines’ adaptability, accuracy, and efficiency.</p>
</section>
<section id="project-impact-and-real-world-applications" class="level2">
<h2 class="anchored" data-anchor-id="project-impact-and-real-world-applications">Project Impact and Real-World Applications</h2>
<p>The solution we developed had a tangible impact on the production process. By implementing advanced vision recognition and dimension measurement technologies, we reduced errors in assembly, improved overall efficiency, and allowed for quicker adaptation to new bottle designs. This not only cut down on waste but also optimized production speed, directly translating to cost savings for the factory.</p>
<p>Beyond this specific application, the techniques we employed—particularly the integration of Mask R-CNN or SAM together with ClipSeg—can be applied to other industries. From automotive to consumer electronics, any field that relies on robotic precision can benefit from the advancements made in this project.</p>
</section>
<section id="background-and-acknowledgement" class="level2">
<h2 class="anchored" data-anchor-id="background-and-acknowledgement">Background and Acknowledgement</h2>
<p>Before we dive into the details and theory, I’d like to take a moment to acknowledge the incredible tools that made this project possible:</p>
<ul>
<li><strong>Matterport, Inc.</strong> for developing the Mask R-CNN model and making it so accessible. Their <a href="https://github.com/matterport/Mask_RCNN">repository</a> and <a href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46">tutorial</a> are great starting points for anyone interested in working with Mask R-CNN.</li>
<li><strong>MetaAI</strong> for the Segment Anything Model (SAM). You can explore their work through their <a href="https://github.com/facebookresearch/segment-anything">repository</a> and even try out the model in action <a href="https://segment-anything.com/demo">here</a>.</li>
<li><strong>Hugging Face</strong> for providing an insightful tutorial on CLIPSeg. If you’re curious about how to use this model, check out their tutorial <a href="https://huggingface.co/blog/clipseg-zero-shot">here</a>.</li>
</ul>
<p>n computer vision, <strong>image classification</strong> involves identifying and labeling the entire image with a single category. <strong>Object localization</strong> goes a step further by not only identifying the objects within an image but also pinpointing their locations using bounding boxes. <strong>Semantic segmentation</strong> assigns a class label to every pixel in an image, grouping pixels that belong to the same object or region but without distinguishing between multiple instances of the same object. <strong>Instance segmentation</strong> combines both tasks, distinguishing and segmenting each individual object instance within an image, even if they belong to the same class, providing the most detailed level of analysis. For our project, instance segmentation is crucial as it allows us to accurately identify and separate each bottle component, ensuring precise manipulation and measurement on the assembly line.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="jargon.jpeg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Overview of image classification, object localization, semantic segmentation, and instance segmentation visually.</figcaption>
</figure>
</div>
</section>
<section id="theory-and-approach" class="level2">
<h2 class="anchored" data-anchor-id="theory-and-approach">Theory and Approach</h2>
<section id="mask-r-cnn-precision-in-object-detection-and-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="mask-r-cnn-precision-in-object-detection-and-segmentation">Mask R-CNN: Precision in Object Detection and Segmentation</h3>
<p>Mask R-CNN (MRCNN) excels in both object detection and generating precise segmentation masks, crucial for dimensional analysis. By leveraging a pre-trained ResNet101 backbone on the COCO dataset, we sped up training while enhancing accuracy. Data augmentation techniques, such as flips and rotations, helped prevent overfitting. Despite its strengths, MRCNN requires substantial labeled data, which can be labor-intensive. Given the specific needs of our project—detecting bottle components like pumps and stems—training and fine-tuning MRCNN proved to be the most effective approach. We used IoU and recall metrics to ensure high segmentation accuracy, directly impacting the precision of our dimensional measurements.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mrcnn_theoy.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Mask R-CNN enhances Faster R-CNN by adding a mask prediction branch and using RoIAlign for precise pixel alignment. It decouples mask and class predictions, improving instance segmentation accuracy.</figcaption>
</figure>
</div>
</section>
<section id="clipseg-sam-leveraging-advanced-segmentation-models" class="level3">
<h3 class="anchored" data-anchor-id="clipseg-sam-leveraging-advanced-segmentation-models">CLIPSeg + SAM: Leveraging Advanced Segmentation Models</h3>
<p>We combined the strengths of CLIPSeg and SAM to achieve optimal segmentation. CLIPSeg, pre-trained on 400 million image-text pairs, allows for zero-shot image segmentation using text prompts, making it versatile. However, its lower resolution can result in blurred boundaries, less ideal for precise measurements. SAM, on the other hand, excels in high-precision segmentation with its extensive training on a diverse dataset of 11 million images. While SAM lacks text-prompt capabilities, it provides superior accuracy. By integrating CLIPSeg’s text-based identification with SAM’s precision, we achieved more accurate and flexible segmentation.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="clipseg_architecture.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">CLIPSeg combines a vision transformer with text prompts for zero-shot segmentation, enabling flexible and precise object identification by leveraging large-scale image-text pairs.</figcaption>
</figure>
</div>
</section>
<section id="pixel-ratio-conversion-translating-pixels-to-real-world-dimensions" class="level3">
<h3 class="anchored" data-anchor-id="pixel-ratio-conversion-translating-pixels-to-real-world-dimensions">Pixel Ratio Conversion: Translating Pixels to Real-World Dimensions</h3>
<p>To convert bounding box measurements into real-world dimensions, we used pixel ratio conversion equations. By establishing a pixel-to-actual-dimension ratio using the known diameter of a sticker, we could accurately translate the height and width of the components into centimeters, ensuring precise dimensional predictions crucial for quality control in the assembly process.</p>
</section>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>The implementation involved several key steps:</p>
<p><strong>Data Collection and Processing:</strong> We gathered a diverse dataset of images containing various objects to train and validate the models. The dataset included images with different lighting conditions, backgrounds, and object orientations. Labelling was done using <a href="https://segments.ai/">Segments.ai</a>.</p>
<p><strong>Model Training and Fine-Tuning:</strong> Mask R-CNN, SAM, and ClipSeg were trained and fine-tuned on the collected dataset. Each model was optimized to perform specific tasks—Mask R-CNN for object detection and segmentation, SAM for versatile segmentation, and ClipSeg for semantic understanding.</p>
<p><strong>Measurement:</strong> Measure bottle components dimension using mathematical transformations which is essentially pixel-ratio-dimensions.</p>
<p><strong>Evaluation:</strong> Assess model performance using metrics such as Intersection over Union (IoU) and recall. Evaluate dimension measurement performance using metrics such as percentage error.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="general_workflow.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Images with diverse conditions were labeled using Segments.ai and processed through two approaches: training and fine-tuning Mask R-CNN; and applying zero-shot segmentation with ClipSeg + SAM. Bottle component measurements were derived from pixel-to-dimension ratios, and model performance was evaluated using IoU, recall, and percentage error metrics.</figcaption>
</figure>
</div>
</section>
<section id="a-comparative-analysis-mrcnn-vs-clipseg-sam" class="level2">
<h2 class="anchored" data-anchor-id="a-comparative-analysis-mrcnn-vs-clipseg-sam">A Comparative Analysis: MRCNN vs CLIPSeg + SAM</h2>
<p>We used the Mean Average Percentage Error (MAPE) to evaluate the performance of MRCNN versus the CLIPSeg + SAM combination. CLIPSeg + SAM consistently outperformed MRCNN in predicting bottle dimensions, with lower percentage errors in height (2.35% vs.&nbsp;9.96%) and width (3.22% vs.&nbsp;12.2%). Despite some variability, CLIPSeg + SAM demonstrated superior accuracy overall. Improving image quality is expected to further enhance this combination’s precision, leading to even greater accuracy in future applications.</p>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mask_rcnn_results.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Segmentation results from the fine-tuned Mask R-CNN model.</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="clipseg_result.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">CLIP output (Left) to be cross-referenced with SAM’s outputs (small pictures to its right). Numbers below are the IoU scores</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
<section id="challenges-and-learnings" class="level2">
<h2 class="anchored" data-anchor-id="challenges-and-learnings">Challenges and Learnings</h2>
<p>The dataset size significantly impacted MRCNN’s performance. Expanding the dataset would likely improve the model’s generalization ability. Additionally, using higher-quality images with clear component boundaries could enhance segmentation accuracy. Incorporating a cropping step in the pre-processing pipeline might help focus the model on relevant areas, potentially improving performance. However, these enhancements were beyond the current project scope due to time and resource constraints.</p>
</section>
<section id="future-work-and-opportunities-for-improvement" class="level2">
<h2 class="anchored" data-anchor-id="future-work-and-opportunities-for-improvement">Future Work and Opportunities for Improvement</h2>
<p>Looking ahead, there are several opportunities to extend this work. Expanding the dataset, incorporating new models, or experimenting with different architectures could yield even better results. Furthermore, the integration of additional features like real-time processing or multi-view analysis could enhance the system’s capability and applicability across various industries.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>The “Robotic Arm Vision Recognition” project showcased the transformative potential of advanced computer vision techniques in enhancing robotic systems. By integrating Mask R-CNN, Segment Anything Model (SAM), and CLIPSeg, we achieved significant improvements in object detection, segmentation, and classification. This project underscores the importance of leveraging state-of-the-art technologies to tackle complex challenges in computer vision and robotics.</p>
<p>As technology continues to advance, integrating such innovative models will undoubtedly lead to even more capable and intelligent robotic systems. The insights gained from this project provide a solid foundation for future developments in robotic vision technology.</p>


</section>

<a onclick="window.scrollTo(0, 0); return false;" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{voon2024,
  author = {Voon, Sharon},
  title = {Robotic {Arm} {Vision} {Recognition}},
  date = {2024-08-28},
  url = {https://s-voon.github.io/blogs/robotic_arms_vision_recognition/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-voon2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Voon, Sharon. 2024. <span>“Robotic Arm Vision Recognition.”</span>
August 28, 2024. <a href="https://s-voon.github.io/blogs/robotic_arms_vision_recognition/">https://s-voon.github.io/blogs/robotic_arms_vision_recognition/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2024, Sharon Voon</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/s-voon">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/sharonvoon/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>