[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sharon Voon",
    "section": "",
    "text": "A dedicated Master of Data Science graduate at the University of British Columbia. Passionate about data modelling and machine learning, I find joy in tackling challenging tasks that push the boundaries of my knowledge. With my degree in Chemical and Biological Engineering, I bring a unique blend of analytical thinking and engineering background to the field of data science. Outside of my academic pursuits, I love immersing myself in the beauty of nature through short hikes and find solace in the world of music and gaming. Let’s connect and explore opportunities to collaborate!\n\n  \n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Sharon Voon",
    "section": "Background",
    "text": "Background\n\n\n\n\nEducation\n\n\nMaster in Data Science\n\n\nB.A.Sc in Chemical and Biological Engineering\n\n\n\n\n\nExperience\n\n\n2+ Years\n\n\nData Associate"
  },
  {
    "objectID": "index.html#data-science-toolkit",
    "href": "index.html#data-science-toolkit",
    "title": "Sharon Voon",
    "section": "Data Science Toolkit",
    "text": "Data Science Toolkit\n\nProgramming Languages: Python, R, SQL\nData Visualization: Dash, ggplot, Matplotlib, Plotly, PowerBI, Shiny\nStatistical Modeling: A/B Testing, Hypothesis Testing\nMachine Learning: Regression, Classification, Clustering, Deep Learning Neural Networks, Recommendation Systems\nSoftware and Packages: Pandas, PyTorch, TensorFlow, Docker, Quarto, Git"
  },
  {
    "objectID": "index.html#areas-of-interest",
    "href": "index.html#areas-of-interest",
    "title": "Sharon Voon",
    "section": "Areas of Interest",
    "text": "Areas of Interest\n\n\n\n\nMachine Learning\n\n\nUtilizes advanced algorithms to solve complex problems and predict outcomes from data.\n\n\n\n\n\nData Analytics\n\n\nAnalyzes data to identify trends and insights, transforming raw data into actionable recommendations.\n\n\n\n\n\nData Visualization\n\n\nCreates visual representations of data to make complex information easily understandable.\n\n\n\n\n\nData Storage\n\n\nManages and organizes data efficiently, ensuring easy access and reliable storage.\n\n\n\n\n\nStatistical Analysis\n\n\nApplies statistical methods to test hypotheses and evaluate data-driven decisions.\n\n\n\n\n\nCloud Compute\n\n\nLeverages cloud services to scale applications and streamline development processes."
  },
  {
    "objectID": "index.html#technical-projects",
    "href": "index.html#technical-projects",
    "title": "Sharon Voon",
    "section": "Technical Projects",
    "text": "Technical Projects\n\n\n\n\n\n\nMachine Learning\n\n\nRobotic Arm Vision Recognition\n\n\nEmployed computer vision algorithms through utilizing zero-shot learning transformers and fine-tuned MRCNN to enable robotic arms for precise object detection and dimensional measurement.\n\n\nRead more\n\n\n\n\n\n\n\nDashboard\n\n\nVancouver Crime Tracker 2023 Dashboard\n\n\nAn interactive dashboard using Dash to visualize 2023 crime data in Vancouver neighborhoods through geolocated maps, trend lines, and bar charts.\n\n\n\nGithub Demo\n\n\n\n\n\n\n\n\nPython Package\n\n\nautopredictor\n\n\nAutopredictor simplifies machine learning model selection for regression tasks. This Python package provides an intuitive approach, minimizing coding for model exploration.\n\n\n\nGithub Tutorial"
  },
  {
    "objectID": "index.html#certifications",
    "href": "index.html#certifications",
    "title": "Sharon Voon",
    "section": "Certifications",
    "text": "Certifications\n\nGoogle Data Analytics Specialization (May 2023)\nAWS Cloud Quest: Cloud Practitioner (May 2023)\nMicrosoft certified: Azure Data FundamentalsDP-900 (Feb 2023)"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Sharon Voon",
    "section": "Contact Me",
    "text": "Contact Me\n\n\n svoon12@gmail.com\n\n\n s-voon\n\n\n Sharon Voon"
  },
  {
    "objectID": "blogs/waste_classification/index.html",
    "href": "blogs/waste_classification/index.html",
    "title": "Waste Classification with AI",
    "section": "",
    "text": "The inspiration from YVR.\nDuring a recent visit to Vancouver International Airport (YVR), I was impressed by the camera scanner system at the waste bins. It automatically classified the waste I was about to dispose of into categories such as garbage, plastic, or organics. This sparked an idea: Could I build a similar AI-powered system to help enhance waste sorting in various environments? That is exactly what I did the very next day."
  },
  {
    "objectID": "blogs/waste_classification/index.html#project-overview",
    "href": "blogs/waste_classification/index.html#project-overview",
    "title": "Waste Classification with AI",
    "section": "Project Overview",
    "text": "Project Overview\nI embarked on a journey to develop a waste classification system using deep learning, specifically leveraging a pre-trained ResNet34 model. The goal was to create a full end-to-end pipeline that could accurately classify waste into different categories, helping users to sort their waste more effectively and contribute to reducing contamination in recycling streams.\n\n\n\nRecycle bins"
  },
  {
    "objectID": "blogs/waste_classification/index.html#choosing-resnet34-for-the-task",
    "href": "blogs/waste_classification/index.html#choosing-resnet34-for-the-task",
    "title": "Waste Classification with AI",
    "section": "Choosing ResNet34 for the Task",
    "text": "Choosing ResNet34 for the Task\nResNet34, a widely-used architecture in the field of computer vision, is known for its ability to handle the complexities of image classification. By fine-tuning this model, I aimed to adapt it to the specific task of waste classification, even with a limited dataset."
  },
  {
    "objectID": "blogs/waste_classification/index.html#the-process-from-notebook-to-usable-script",
    "href": "blogs/waste_classification/index.html#the-process-from-notebook-to-usable-script",
    "title": "Waste Classification with AI",
    "section": "The Process: From Notebook to Usable Script",
    "text": "The Process: From Notebook to Usable Script\n\nEnvironment Setup\nThe project began with setting up the necessary environment. Using PyTorch, I ensured that the appropriate libraries were installed and CUDA was available for GPU acceleration. This setup was critical for speeding up the model training process.\nData Preparation and Augmentation\nI started by preparing the dataset, which included images of various waste items like plastics, organics, and general garbage. The images were preprocessed using transformations such as resizing, normalization, and data augmentation. These techniques included random rotations, flips, and color jittering to increase the diversity of the training data, thereby improving the model’s generalization capability.\nHere’s a snippet of the code used for data transformations:\ntransform = transforms.Compose([\ntransforms.Resize((224, 224)),\ntransforms.RandomHorizontalFlip(),\ntransforms.RandomRotation(10),\ntransforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\ntransforms.ToTensor(),\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\nFine-Tuning the ResNet34 Model\nNext, I moved on to fine-tuning the ResNet34 model. The pre-trained ResNet34 model, which was originally trained on the ImageNet dataset, was adapted to classify waste. This involved:\n\nReplacing the final fully connected layer to match the number of waste - categories.\nFreezing the earlier layers of the model to retain the pre-trained weights.\nFine-tuning the last two layers to adapt the model to the specific task.\n\n# Load the pre-trained ResNet34 model\nmodel = models.resnet34(pretrained=True)\n\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace the final layer for binary classification\nnum_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Linear(num_features, 250),\n    nn.ReLU(),\n    nn.Linear(250, 2)\n)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\nTraining and Evaluation\n\nI then proceeded to train the model using the prepared dataset. The training process involved multiple epochs, where the model’s performance was continuously monitored on the validation set. The training loop included the following steps:\n\nForward pass: The model made predictions on the input data.\nBackward pass: The loss was computed, and gradients were calculated.\nOptimization: The model’s weights were updated to minimize the loss.\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n\n# Evaluate the model\nmodel.eval()\naccuracy = 0.0\nwith torch.no_grad():\n    for images, labels in val_loader:\n        outputs = model(images)\n        _, preds = torch.max(outputs, 1)\n        accuracy += torch.sum(preds == labels.data).item()\n\naccuracy = accuracy / len(val_loader.dataset)\nprint(f'Validation Accuracy: {accuracy * 100:.2f}%')\n\n\nBuilding an End-to-End Pipeline\n\nTo make the workflow efficient and reproducible, I developed an ETL (Extract, Transform, Load) pipeline, automating the key steps of data extraction, transformation, model training, evaluation, and prediction. This pipeline was encapsulated into a series of scripts, each handling a specific stage of the process.\n\n\n\nData Pipeline fo the project.\n\n\n\nData Extraction: Downloading Data from Kaggle\n\nThe first step in the pipeline was to extract the dataset. I used the Kaggle API to download the necessary waste classification dataset. By automating this process, I ensured that the latest data could always be accessed and used for model training. This script authenticates with Kaggle using the API, downloads the specified dataset, and extracts it to a designated directory.\n\nData Transformation and Model Training\n\nAfter downloading the dataset, the next step was to transform the data and train the model. The transformation process involved image resizing, normalization, and augmentation to prepare the data for training. The training script used PyTorch to fine-tune a ResNet34 model on the transformed dataset. This script automates the model training process, from loading the dataset to saving the trained model. It can be executed with minimal setup, making it easy to retrain the model as needed.\n\nModel Evaluation\n\nOnce the model was trained, I needed to evaluate its performance on unseen data. The evaluation script loads the trained model and runs it against a validation dataset to assess its accuracy and other metrics. This script ensures that the model’s performance is quantified and can be used to decide whether further training or tuning is required.\n\nPrediction: Classifying New Waste Images\n\nThe final stage of the pipeline involves using the trained model to classify new images. This script takes an image as input, preprocesses it, and outputs the predicted waste category. This script allows users to input a new image, and the model will output the corresponding waste category. It’s a straightforward yet powerful tool that can be easily integrated into a larger waste management system."
  },
  {
    "objectID": "blogs/waste_classification/index.html#conclusion",
    "href": "blogs/waste_classification/index.html#conclusion",
    "title": "Waste Classification with AI",
    "section": "Conclusion",
    "text": "Conclusion\nThis project began as a curiosity sparked by an AI system at an airport and evolved into a comprehensive waste classification pipeline. By fine-tuning a ResNet34 model, I developed a robust classifier that can assist in sorting waste more effectively. This project not only contributes to environmental sustainability but also showcases the power of AI in solving real-world problems.\nThe end-to-end pipeline I created can be easily deployed in various settings, from airports to office buildings, making it a versatile tool for improving waste management practices. With further refinements and a larger dataset, this system could play a significant role in reducing waste contamination and promoting a cleaner, more sustainable world."
  },
  {
    "objectID": "blogs/waste_classification/index.html#access-the-full-project-on-github",
    "href": "blogs/waste_classification/index.html#access-the-full-project-on-github",
    "title": "Waste Classification with AI",
    "section": "Access the Full Project on GitHub",
    "text": "Access the Full Project on GitHub\nIf you’re interested in exploring the full codebase or contributing to the project, you can access the complete repository on GitHub. The repository includes all the scripts mentioned above, detailed documentation, and instructions on how to set up and run the project.\nVisit the Waste Classification Project Repository on GitHub to dive into the code, download the project, and start classifying waste with AI."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html",
    "href": "blogs/diabetes_classification/index.html",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "",
    "text": "Embark on a journey with me as I share insights into creating a predictive diabetes classification model. Learn how to streamline your workflow by hosting a virtual environment using Docker, ensuring seamless software dependency management. Drawing from a group project during my MDS studies, where we explored decision tree and kNN algorithms with the 2015 BFRSS dataset, I’ll guide you through the process of developing scripts and analysis pipeline for non-interactive data analysis. While I won’t delve into the projects findings here, you can explore the complete source code and report here."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#introduction",
    "href": "blogs/diabetes_classification/index.html#introduction",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "",
    "text": "Embark on a journey with me as I share insights into creating a predictive diabetes classification model. Learn how to streamline your workflow by hosting a virtual environment using Docker, ensuring seamless software dependency management. Drawing from a group project during my MDS studies, where we explored decision tree and kNN algorithms with the 2015 BFRSS dataset, I’ll guide you through the process of developing scripts and analysis pipeline for non-interactive data analysis. While I won’t delve into the projects findings here, you can explore the complete source code and report here."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#demystifying-docker-a-high-level-introduction-to-containerization",
    "href": "blogs/diabetes_classification/index.html#demystifying-docker-a-high-level-introduction-to-containerization",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Demystifying Docker: A High-Level Introduction to Containerization",
    "text": "Demystifying Docker: A High-Level Introduction to Containerization\n\n\n\nWhat is Conatinerization?\n\n\nContainers serve as a power solution for creating isolated computational environments. Distinct from virtual environments, they offer enhances isolation from the host operating system and provide a versatile platform for sharing various software, applications, and operating system dependencies. One of the most prevalent questions that often arises is: What is Docker, and why choose Docker?\nDocker, as a containerization software, is a platform meticulously crafted to empower developers in constructing, sharing, and executing container applications, thereby eliminating the hassles associated with intricate environment setups. We’ve all been through the common yet exasperating scenario in collaborative projects: ‘It works on my end. I am not sure why it isn’t working on your laptop.’\nDocker serves as a solution to this issue by allevating the burden of meticulous environment configuration and management. Furthermore, it seamlessly integrates with your existing tools, such as your GitHub repository, offering a consistent and reliable approach to containerized applications."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#installing-docker",
    "href": "blogs/diabetes_classification/index.html#installing-docker",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Installing Docker",
    "text": "Installing Docker\n\nTo get started, create a free Docker account here.\nOnce you have signed up and logged into Docket store, proceed to install Docker for your operating system. You can find the installtion instructions for windows here. Follow the steps outlines on the official page for a seamless installation experience."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#quick-tutorial-running-a-simple-container-example",
    "href": "blogs/diabetes_classification/index.html#quick-tutorial-running-a-simple-container-example",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Quick Tutorial: Running a Simple Container Example",
    "text": "Quick Tutorial: Running a Simple Container Example\nOne of the many benefits of container is their ability to provide a platform for sharing various software. Here we will utilize Docker to run a container containing the RStudio server web-application. Ensure that you are signed into your Docker Desktop before proceeding.\ndocker run --rm -p 8787:8787 -e PASSWORD=\"yourpassword\" rocker/rstudio:4.3.2\n\nBreakdown of the command\n\nThe -p flag informs Docker that a port will be used to access RStudio in your web browser, with the specified location as 8787:8787.\nThe -rm option ensures that when you exit the container, it is automatically deleted, preventing manual removal and saving disk space.\nThe -e flag is used to set environment variables within the container. Environment variables are key-value pairs that can be passed to a Docker container providing a way to configure the containerized application or modify its behaviour based on these varibles. -e PASSWORD=\"yourpassword\" sets the password for logging into the RStudio web app as ‘yourpassword’.\n\nIf this is your first time running this Docker container, Docker will automatically search for the container on DockerHub (equivalent to GitHub but for Docker images) and download it if it exists. The specified version 4.3.2, indicates the specific RStudio version to download from DockerHub.\nYou can now access your local web app at http:\\\\localhost:8787. Log into RStudio web app using: ‘rstudio’ as the username and ‘yourpassword’ as the password."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#building-the-docker-image-from-a-widely-used-dockerhub-container-registry",
    "href": "blogs/diabetes_classification/index.html#building-the-docker-image-from-a-widely-used-dockerhub-container-registry",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Building the Docker Image from a Widely-Used DockerHub Container Registry",
    "text": "Building the Docker Image from a Widely-Used DockerHub Container Registry\nA container registry serves as a remote repository or collection of repositories for sharing container images. Notable container registries include DockerHub, Quay, AWS, and more. In this tutorial, we will focus on DockerHub.\nIt is a common and recommended practice to construct a container image tailored to your project atop a base container image. Utilizing a base image ensures a clean and reproducible environment, facilitating effective dependency management. The advantages extend beyond consistensy, efficiency, reproducibility, and security.\nFor our group project, coded in Python Language, we opted for a Jupyter minimal-notebook image from quay.io, equipped with ipykernel, ipython, and jupyterLab for a dynamic notebook experience. To integrate this base image, a DockerFile is required. A Dockerfile serves as a script guiding the creation of a Docker image offering instructions for Docker to follow in building a container image specific to an application or service. The primary purposes of a Dockerfile are to define the base image and install specific dependencies. It is advisable to specify version numbvers for dependencies to prevent potential clashes in the future.\n\n\n\nDockerfile\n\n\n\nBreakdown of the Dockerfile\nOur Dockerfile is structured as follows:\nFROM quay.io/jupyter/minimal-notebook:2023-11-19\n\n# base image comes with ipykernel, ipython, jupyterLab\n\nRUN conda install -y pandas=2.1.3 \\\n    altair=5.1.2 \\\n    scikit-learn=1.3.2 \\\n    vegafusion=1.4.5 \\\n    vegafusion-python-embed \\\n    click=8.1.7 \\\n    jupyter-book=0.15.1 \\\n    make=4.3\n\nRUN pip install vl-convert-python==1.1.0 \\\n    pytest==7.4.3 \\\n    ucimlrepo==0.0.3 \\\n    myst-nb==0.17.2\nA typical Dockerfile commences with a FROM command, specifying the base image upon which the new image will be built off. Docker images are constructed in layers to maintain a lightweight profile. The subsequent RUN commands install new software or execute configuration commands. To enhance redability and organization, the installation process is segmented based on the channels from which dependencies are sourced. In this instance, dependencies installed through conda are grouped together, while those installed via pip are managed separately. The -y flag is essential for the conda command, requiring approval of dependencies before installation when using the conda channel.\nTo ensure the Dockerfile functions as intended, we verify its functionality by building the image locally. Run the following command from the directory where the Dockerfile is situated:\ndocker build --tag diabetes_classification:v1 .\ndocker run --rm -it diabetes_classification:v1\n\nThe --tag flag designates the name of the Docker Image. Here, it is set to ‘test1’ with a version number of ‘v1’\nThe . denotes the current working directory, indicating that the Dockerfile is located in the same directory where the command is executed\n\nPress Ctrl + C to shut down this jupyter notebook."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#launching-docker-image-in-a-container",
    "href": "blogs/diabetes_classification/index.html#launching-docker-image-in-a-container",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Launching Docker Image in a container",
    "text": "Launching Docker Image in a container\nLaunching the image as a container typically involves using the docker run command. However, this command can become complex with multiple flags for environment configuration, making it error-prone. To simplify this process and enhance managemability, the docker-compose.yml file is employed to define how the container should be launched.\nTo initiate the container interactively using the docker-compose.yml file, execute the following command: docker-compose up. This ensures a streamlined and error-resistant approach to container deployment.\n\nBreakdown of the YAML file\nOur docker-compose.yml is structured as follows:\nservices:\n  jupyter:\n    image:  diabetes_classification:v1\n    volumes:\n      - .:/home/jovyan\n    ports:\n      - 8888:8888\n    deploy:\n      resources:\n        limits:\n          memory: 5G\n    platform: linux/amd64\n\nservices: This section defines the services that compose our application.\njupyter: This is the name of our service, representing a Jupyter container.\nimage: diabetes_classification:v1: Specifies the Docker image to use for the jupyter service. It’s tagged as version v1.\nvolumes: Defines the volume mount between the host machine and the container. .-/home/jovyan mounts the current directory (.) to the /home/jovyan directory inside the container. This allows data to be shared between the host and the Jupyter container.\nports: Maps the host machine’s port to the container’s port. 8888:8888 maps port 8888 on the host to port 8888 in the Jupyter container.\ndeploy: Provides additional deployment options for the service.\nresources: Specifies resource limits for the service.\nlimits:: Sets limits on resource usage.\nmemory: 5G: Limits the memory usage of the jupyter service to 5 gigabytes.\nplatform: linux/amd64: Specifies the platform for which the image is built. In this case, it’s configured for the Linux AMD64 architecture.\n\nTo host the container, use the follwing command from the directory where the Dockerfile and docker-compose.yml are situated:\ndocker-compose up\nPress Ctrl + C to gracefully shut down this jupyter notebook.\nCongratulations! You have successfully built a DockerImage and launched it in an isolated container tailored for your specific project."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#sharing-the-docker-image",
    "href": "blogs/diabetes_classification/index.html#sharing-the-docker-image",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Sharing the Docker Image",
    "text": "Sharing the Docker Image\n\n\n\nDockerHub\n\n\nSharing your virtual environment can be done in two ways:\n\nShare the Dockerfile and the docker-compose.yml files with others, allowing them to rerun the same commands above.\nA generally preferred method is to push your image to DockerHub where others can effortlessly download it through DockerHub, similar to how you obtained the base image. To do so, you would need to sign up for a DockerHub account here.\n\n\nPushing the Docker Image to DockerHub:\n\nTag Your Image:\n\n\nBefore pushing your image, tag it with your DockerHub username and the desired repository name.\n\ndocker tag diabetes_classification:dockerhub_username/repository_name:v1\n\nLogin to Docker Hub: -The following command will prompt you to enter your Docker Hub username and password.\n\ndocker login\n\nPush the image to your DockerHub repository using the same tag name in Step 1.\n\ndocker push  dockerhub_username/repository_name:v1\n\nVerify that the image has been successfully pushed by checking the DockerHub website.\nOthers can now download the Docker image through the following command:\n\ndocker run --rm -it dockerhub_username/repository_name:v1\nThis streamlined process ensures seamless sharing and replication of your Docker Image, enhancing collaboration in your project."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#conclusion",
    "href": "blogs/diabetes_classification/index.html#conclusion",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, this tutorial has guided you through the process of building and disseminating a virtual environment tailored for a predictive diabetes classification model using Docker. The utilization of Docker for environment management ensures reproducibility and eridicate compatibility concerns. Whether sharing the Dockerfile and docker-compose.yml for others to replicate the environment or pushing the image to DockerHub for convenient access, these practices enhance collaboration and streamline the deployment of your specific project. Embrace the efficiency and consistency that Docker provides, empowering your data science endeavors with a robust and shareable containerized solution.\nAdditionally, it’s worth nothing that in this project, we implemented a GitHub workflow to automate the building and pushing of updated Dockerfile to our DockerHub which is not covered in this tutorial. This further streamline the process, reinforcing the central concept."
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#github-source-code",
    "href": "blogs/diabetes_classification/index.html#github-source-code",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "GitHub Source Code",
    "text": "GitHub Source Code\nGitHub repository"
  },
  {
    "objectID": "blogs/diabetes_classification/index.html#diabetes-model-report",
    "href": "blogs/diabetes_classification/index.html#diabetes-model-report",
    "title": "Diabetes Classification Model: Virtual Environment through Docker",
    "section": "Diabetes model Report",
    "text": "Diabetes model Report\nAnalysis Report"
  },
  {
    "objectID": "blogs/autopredictor_package/index.html",
    "href": "blogs/autopredictor_package/index.html",
    "title": "Autopredictor: Python Package",
    "section": "",
    "text": "Welcome to the world of Python packages! If you’ve embarked on your coding journey using the Pyton language, chances are you’ve encountered Python packages. Some widely-used examples are numpy, pandas, and scikit-learn for machine learning models. In this tutorial, we’ll delve into the process of crafting your own Python package and publishing it on the Python Package Index(PyPI), a platform designed for hosting python packages. This enables others to seamlessly leverage your work with a simple import statement.\nPython packages offer a structured and modular approach to organizing code, bundling related functionalities, classes, and resources. This results in a well-organized and scalable codebase, simplifying development, maintenance, and collaboartion –essential tools for Python developers.\n\n\n\n\n\nPackage\n\n\n\nModularity: Packages help break down complex projects into manageable and modular units, fostering a more organized code structure. This modularity also simplifies writing test cases targetting specific sections and functions.\nReusability: Once written, packages can be reused across various projects, saving time and effort by leveraging existing code.\nMaintainanility: Packages facilitate easier maintenance and updates, allowing developers to focus on specific parts of a project without affecting the entire codebase.\nDependency Management: Packages enable parallel development, where team members can work independently on different modules, promoting collaboration and efficiency.\nVersion Control: Packages support versioning, ensuring compatibility and making it easier to manage changes in a project over time.\n\nUnderstanding and harnessing the power of Python packages is a fundamental skill for any Python developer. So, let’s dive in and explore how to create, use, and benefit from these modula building blocks in your Python projects!"
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#introduction",
    "href": "blogs/autopredictor_package/index.html#introduction",
    "title": "Autopredictor: Python Package",
    "section": "",
    "text": "Welcome to the world of Python packages! If you’ve embarked on your coding journey using the Pyton language, chances are you’ve encountered Python packages. Some widely-used examples are numpy, pandas, and scikit-learn for machine learning models. In this tutorial, we’ll delve into the process of crafting your own Python package and publishing it on the Python Package Index(PyPI), a platform designed for hosting python packages. This enables others to seamlessly leverage your work with a simple import statement.\nPython packages offer a structured and modular approach to organizing code, bundling related functionalities, classes, and resources. This results in a well-organized and scalable codebase, simplifying development, maintenance, and collaboartion –essential tools for Python developers.\n\n\n\n\n\nPackage\n\n\n\nModularity: Packages help break down complex projects into manageable and modular units, fostering a more organized code structure. This modularity also simplifies writing test cases targetting specific sections and functions.\nReusability: Once written, packages can be reused across various projects, saving time and effort by leveraging existing code.\nMaintainanility: Packages facilitate easier maintenance and updates, allowing developers to focus on specific parts of a project without affecting the entire codebase.\nDependency Management: Packages enable parallel development, where team members can work independently on different modules, promoting collaboration and efficiency.\nVersion Control: Packages support versioning, ensuring compatibility and making it easier to manage changes in a project over time.\n\nUnderstanding and harnessing the power of Python packages is a fundamental skill for any Python developer. So, let’s dive in and explore how to create, use, and benefit from these modula building blocks in your Python projects!"
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#building-a-python-package-from-scratch",
    "href": "blogs/autopredictor_package/index.html#building-a-python-package-from-scratch",
    "title": "Autopredictor: Python Package",
    "section": "Building a Python package from scratch",
    "text": "Building a Python package from scratch\nThrough this guide, I’ll be guide you through the process of creating a Python package from the ground up, using the autopredictor package. As a contributor to the collaborative team behind this project, our goal is to simplify the repetitive task of regression model selection and comparision within the machine learning workflow.\nThe autopredictor package accelerates the exploration of eight regression models by evaluating them with default settings across four socring metrics. This approach allows users to quickly grasp the performance of each model. autopredictor caters to both beginners by eliminating the need for complicated model arguments and to experts by providing baseline results. You can find the source code of this package here and the vingette documentation here.\n\nSource Code Development\nBefore getting into the intricacies of package creationg, it is essential to develop the code we intend to transform into a package. This initial step aids in organizing the code into distinct functions, forming the foundation for the subsequent packaging process. In this context, we refer to the core source code equivalent to those found under the src/autopredictor folder in this package repository. Once you have this code prepared, we can seamlessly process to establish the structure of our package.\n\n\nPackage structure\nA python package conventionally adheres to a specific structure, emphasizing the advantages of modularity and simplicity. The structure for the autopredictor package is outlined below, with files neatly categorized into three main groups:\n\nPackage documentation:\n\n\nREADME.md: Provides essentioal information and instructions for users, typically a summary of what the package achieves.\ndocs/: Contains additinal documentation files or folder for example vingette.\npyproject.toml: Instruction on how to build and install the package on a computer.\n\n\nPackage source code:\n\n\n__init__.py: marks the directory as a Python package.\nModule files (.py): Houeses the core functinoalities, classes, and utilities.\n\n\nPackage tests\n\n\ntest/: Contains files for testing the package, ensuring functinality and realiability.\n\n\nCheckpoint:\nMake sure that you have installed the necessary tools, namely cookiecutter and poetry, as these will be integral to the package-building process in this tutorial. Follow the Poetry installation instructions for the poetry package. For cookiecutter, execute the following command in your terminal:\nconda install -c conda-forge cookiecutter\n\n\n\ncookiecutter template\n\n\n\nCookiecutter\n\n\nOne easy way to set up the structure is through leveraging the cookiecutter tool. cookiecutter is a easy-to-use tool for populating a directory structure from a pre-made template for different projects, including and not limited to creating python packages, R packages,and websites. I chose to use this specific py-pkgs-cookiecutter template that is tailored for creating Python package.\nTo create a package directory structure using this template, execute the following command from the directory where you want to establish your package.\ncookiecutter https://github.com/py-pkgs/py-pkgs-cookiecutter.git\nThis command initiates a template and prompts you for information related to your package. Thesse details will be automatically incorporated into the respective files within the package. During this process, opt for no when prompted about ‘GitHub Actions’ as the focus here is solely on developing the package structure, not automation. The image below displays the responses provided during these prompts:\n\n\n\ncookiecutter prompts’ response for autopredictor package\n\n\nAfter completing these prompts, you will find a newly created directory in your working directory named autopredictor.\n\n\nSetting up remote version control\nNow is an opportune moment to establish a GitHub repository for version control. Begin by creating a new repository under your preferred GitHub account or organization. Follow the instructions provided on GitHub to link it with the package directory you recently generated using cookiecutter. This integration will facilitate version tracking and collaborative development for your package.\n\n\nIntegrating the Developed source code into the Package Directory\nPlace all the developed source code within modules in the src/autopredictor/ directory. Although the cookiecutter has automatically created a module named autopredictor.py, you have the flexibility to organize your functions under a single module or distribute them across multiple modules based on your preference."
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#installing-your-package",
    "href": "blogs/autopredictor_package/index.html#installing-your-package",
    "title": "Autopredictor: Python Package",
    "section": "Installing your package",
    "text": "Installing your package\nSeveral tools are available for delveloping installable Python packages, such as poetry, flit, and setuptools. In this tutorial, we will utilize poetry, a modern packaging tool that offers straightforward and efficient commands for developing, installing, and distributing Python packages.\nThe central configuration file for a poetry managed package is the pyproject.toml file. This file contains all the metadata and installation instructions for the package. During the setup, cookiecutter has automatically generated a base pyproject.toml file, which is organized into four main sections:\n\n[tool.poetry]: Defines the package metadata.\n[tool.poetry.dependencies]: Lists of dependencies required for general users.\n[tool.poetry.group.dev-dependencies]: Specifies dependencies required for developers.\n[build-system]: Identifies the build tools required to construct the package.\n\nIt is crucial to create a new virtual environment dedicated to your package to prevent potential conflicts. To establish a new virtual enviuronment with python, utilize the command conda create --name autopredictor python=3.9 -y. To acitvate the environment, execute the command conda activate autopredictor. This ensures a clean and isloated environment for your package installation. To install your package, run the command poetry install from the directory where the pyrpoject.toml file is located.\n\nAdding dependencies to your package\n\n\n\nDependencies Management\n\n\nIf your package relies on additional dependencies, incorporating them into the pyproject.toml file with poetry is straightforward. You have two options: manual addition or utilizing commands.\nFor instance, the autopredictor package utilizes scikit-learn as one of its dependencies. Use the command poetry add scikit-learn to effortlessly include this dependency in your environemnet.The scikit-learn entry will be automatically appended to the pyproject.toml file under the [tool.poetry.dependencies] section.\nIf you prefer the manual approach, simply add scikit-learn = \"^1.3.2\" under the [tool.poetry.dependencies] section in the pyproject.toml file directly. This sets a version constraint, specifying that the package requires version 1.3.2 or any higher version of scikit-learn. poetry will automatically generates a poetry.lock file which serve as a recod of all the exact version of the dependencies used in a project during installation, removal, or updating of any dependency."
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#testing-code-coverage-and-documentation",
    "href": "blogs/autopredictor_package/index.html#testing-code-coverage-and-documentation",
    "title": "Autopredictor: Python Package",
    "section": "Testing, Code Coverage, and Documentation",
    "text": "Testing, Code Coverage, and Documentation\nWhile we won’t delve into extensive details on these sub-topics, it is imperative to create robust test units for the functions in your package and achieve comprehensive code coverage. To integrate pytest for developing unit tests, execute the command poetry add --group dev pytest pytest-cov. This command automatically updates the [tool.poetry.group.dev-dependencies] section in the pyproject.toml file. Don’t forget to commit and push these changes to your repository.\nCrafting a well-documented example vignette is essential for users to comprehend how your package functions and troubleshoot effectively. This documentation not only enhances the usability of your package but also contributes to a more user-friendly and collaborative development environment."
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#building-and-publishing-your-package-on-pypi",
    "href": "blogs/autopredictor_package/index.html#building-and-publishing-your-package-on-pypi",
    "title": "Autopredictor: Python Package",
    "section": "Building and publishing your package on PyPI",
    "text": "Building and publishing your package on PyPI\n\n\n\nPyPI\n\n\nWith all the components in place from the previous section, your package is now poised for building and publishing. Building entails creating a ‘distribution package,’ a unified archive file encompassing all the necessary files and information for package installation via tools like pip. Users can easily install your package using a command such as pip install autopredictor. Distribution packages can take the form of sdists or wheels.\nTo generate your ‘distribution package,’ execute the command poetry build from your root package directory. This will generate a new directory named dist/. Users can install your package by accessing this distribution package.\nTo further streamline the process, you can publish your distribution package on PyPI. Begin by signing up for a PyPI account and create a new API token for PyPI authentication. Add your API token to Poetry and publish your package using the following commands:\npoetry config pypi-token.pypi your-api-token\npoetry publish --build\nThese steps simplify the distribution and accessibility of your package, making it readily available for the broader Python community."
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#continuous-integration-and-continuous-deployment-cicd-for-autopredictor",
    "href": "blogs/autopredictor_package/index.html#continuous-integration-and-continuous-deployment-cicd-for-autopredictor",
    "title": "Autopredictor: Python Package",
    "section": "Continuous Integration and Continuous Deployment (CI/CD) for autopredictor",
    "text": "Continuous Integration and Continuous Deployment (CI/CD) for autopredictor\nIn modern software development, CI/CD pipelines are crucial for ensuring that code changes are automatically tested, validated, and deployed. Implementing CI/CD for your Python package helps maintain high-quality code, streamline the development process, and minimize manual intervention during deployments. In the autopredictor project, we utilized GitHub Actions to set up a robust CI/CD pipeline that automates testing, coverage tracking, documentation building, and package deployment.\n\nSetting up CI/CD with GitHub Actions\nGitHub Actions is a powerful automation platform that allows developers to define workflows for automating tasks such as testing and deployment. Here’s an overview of how the CI/CD pipeline is configured for the autopredictor package:\n\nContinuous Integration (CI) The CI job is triggered whenever code is pushed to the repository or a pull request is opened. It ensures that the new changes are thoroughly tested and meet the project’s quality standards.\n\n\nSet Up Python Environment: The pipeline begins by setting up a Python environment using Python 3.9.\nInstall Dependencies: The code repository is checked out, and the necessary dependencies are installed using poetry.\nRun Tests with Pytest: Automated tests are run using pytest, with coverage tracking enabled to ensure the codebase is well-tested.\nUpload Coverage to Codecov: The coverage report is uploaded to Codecov, a tool for tracking and analyzing code coverage over time.\nBuild Documentation: The project documentation is built automatically to ensure it is always up to date.\n\n\nContinuous Deployment (CD) The CD job is responsible for deploying the package once the CI checks have passed. This job only runs when changes are pushed to the main branch, ensuring that only stable code is deployed.\n\n\nPrepare Release with Python Semantic Release: This step automatically determines the next version number based on the commit history, generates release notes, and creates a Git tag.\nPublish to TestPyPI: Before deploying to the official Python Package Index (PyPI), the package is first published to TestPyPI for validation.\nTest Installation from TestPyPI: The package is then installed from TestPyPI to ensure it was published correctly and can be installed by users.\nPublish to PyPI: If all the previous steps succeed, the package is published to PyPI, making it available to the broader Python community.\nPublish to GitHub Releases: The package distributions are also uploaded to GitHub Releases, providing an additional source for users to download the package.\n\n\n\nBenefits of CI/CD in autopredictor\nImplementing CI/CD in the autopredictor project brings several advantages:\n\nAutomated Testing: Ensures that all changes are tested automatically, reducing the risk of introducing bugs.\nConsistent Documentation: Documentation is built and updated automatically, ensuring users always have access to the latest information.\nStreamlined Deployment: The package is automatically published to PyPI and GitHub Releases upon passing all checks, reducing manual deployment efforts.\nImproved Code Quality: Continuous code coverage tracking helps maintain a high level of test coverage, ensuring code quality over time."
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#conclusion",
    "href": "blogs/autopredictor_package/index.html#conclusion",
    "title": "Autopredictor: Python Package",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations on reaching this milestone! By following this tutorial, you have successfully created, developed, and published your first Python package on PyPI. This achievement not only demonstrates your proficiency in package development but also opens up opportunities for collaboration, contribution, and sharing your work with the wider Python community. As you continue your journey in software development, the skills you’ve gained in packaging and distribution will undoubtedly prove valuable. Best of luck with your future projects, and feel free to explore more advanced features and practices in the dynamic world of Python packaging. Happy coding!"
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#git-hub-repository",
    "href": "blogs/autopredictor_package/index.html#git-hub-repository",
    "title": "Autopredictor: Python Package",
    "section": "Git Hub repository",
    "text": "Git Hub repository\nPlease find the source code repository here."
  },
  {
    "objectID": "blogs/autopredictor_package/index.html#usage-documentation",
    "href": "blogs/autopredictor_package/index.html#usage-documentation",
    "title": "Autopredictor: Python Package",
    "section": "Usage Documentation",
    "text": "Usage Documentation\nPlease find the vingette documentation for autopredictor‘s usage example and the functions’ descriptions here."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html",
    "href": "blogs/robotic_arms_vision_recognition/index.html",
    "title": "Robotic Arm Vision Recognition",
    "section": "",
    "text": "Ever wondered how a driverless car like Tesla navigates the world so seamlessly? The secret lies in computer vision—a technology that enables these cars to “see” their surroundings, identifying objects and understanding their exact locations. But this isn’t just limited to autonomous vehicles. Computer vision is transforming industries, making everyday tasks more efficient and precise.\nIn partnership with Analytika, my recent capstone project, “Robotic Arm Vision Recognition,” tapped into the incredible potential of computer vision to revolutionize robotic systems. By harnessing the power of cutting-edge models like Mask R-CNN, Segment Anything Model (SAM), and ClipSeg, we pushed the boundaries of what robotic arms can achieve in a real-world factory setting."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#problem-and-motivation",
    "href": "blogs/robotic_arms_vision_recognition/index.html#problem-and-motivation",
    "title": "Robotic Arm Vision Recognition",
    "section": "Problem and Motivation",
    "text": "Problem and Motivation\nIn modern manufacturing, especially on bottle production lines, efficiency and precision are paramount. The challenge? Traditional methods often fell short in accuracy, leading to increased waste and downtime. Typically, each assembly line is tailored to a specific bottle shape and size, requiring manual adjustments during turnover to accommodate different types. Our mission was clear: to develop an advanced vision recognition system that could handle this variability with precision and automation, thus improving the production lines’ adaptability, accuracy, and efficiency."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#project-impact-and-real-world-applications",
    "href": "blogs/robotic_arms_vision_recognition/index.html#project-impact-and-real-world-applications",
    "title": "Robotic Arm Vision Recognition",
    "section": "Project Impact and Real-World Applications",
    "text": "Project Impact and Real-World Applications\nThe solution we developed had a tangible impact on the production process. By implementing advanced vision recognition and dimension measurement technologies, we reduced errors in assembly, improved overall efficiency, and allowed for quicker adaptation to new bottle designs. This not only cut down on waste but also optimized production speed, directly translating to cost savings for the factory.\nBeyond this specific application, the techniques we employed—particularly the integration of Mask R-CNN or SAM together with ClipSeg—can be applied to other industries. From automotive to consumer electronics, any field that relies on robotic precision can benefit from the advancements made in this project."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#background-and-acknowledgement",
    "href": "blogs/robotic_arms_vision_recognition/index.html#background-and-acknowledgement",
    "title": "Robotic Arm Vision Recognition",
    "section": "Background and Acknowledgement",
    "text": "Background and Acknowledgement\nBefore we dive into the details and theory, I’d like to take a moment to acknowledge the incredible tools that made this project possible:\n\nMatterport, Inc. for developing the Mask R-CNN model and making it so accessible. Their repository and tutorial are great starting points for anyone interested in working with Mask R-CNN.\nMetaAI for the Segment Anything Model (SAM). You can explore their work through their repository and even try out the model in action here.\nHugging Face for providing an insightful tutorial on CLIPSeg. If you’re curious about how to use this model, check out their tutorial here.\n\nn computer vision, image classification involves identifying and labeling the entire image with a single category. Object localization goes a step further by not only identifying the objects within an image but also pinpointing their locations using bounding boxes. Semantic segmentation assigns a class label to every pixel in an image, grouping pixels that belong to the same object or region but without distinguishing between multiple instances of the same object. Instance segmentation combines both tasks, distinguishing and segmenting each individual object instance within an image, even if they belong to the same class, providing the most detailed level of analysis. For our project, instance segmentation is crucial as it allows us to accurately identify and separate each bottle component, ensuring precise manipulation and measurement on the assembly line.\n\n\n\nOverview of image classification, object localization, semantic segmentation, and instance segmentation visually."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#theory-and-approach",
    "href": "blogs/robotic_arms_vision_recognition/index.html#theory-and-approach",
    "title": "Robotic Arm Vision Recognition",
    "section": "Theory and Approach",
    "text": "Theory and Approach\n\nMask R-CNN: Precision in Object Detection and Segmentation\nMask R-CNN (MRCNN) excels in both object detection and generating precise segmentation masks, crucial for dimensional analysis. By leveraging a pre-trained ResNet101 backbone on the COCO dataset, we sped up training while enhancing accuracy. Data augmentation techniques, such as flips and rotations, helped prevent overfitting. Despite its strengths, MRCNN requires substantial labeled data, which can be labor-intensive. Given the specific needs of our project—detecting bottle components like pumps and stems—training and fine-tuning MRCNN proved to be the most effective approach. We used IoU and recall metrics to ensure high segmentation accuracy, directly impacting the precision of our dimensional measurements.\n\n\n\nMask R-CNN enhances Faster R-CNN by adding a mask prediction branch and using RoIAlign for precise pixel alignment. It decouples mask and class predictions, improving instance segmentation accuracy.\n\n\n\n\nCLIPSeg + SAM: Leveraging Advanced Segmentation Models\nWe combined the strengths of CLIPSeg and SAM to achieve optimal segmentation. CLIPSeg, pre-trained on 400 million image-text pairs, allows for zero-shot image segmentation using text prompts, making it versatile. However, its lower resolution can result in blurred boundaries, less ideal for precise measurements. SAM, on the other hand, excels in high-precision segmentation with its extensive training on a diverse dataset of 11 million images. While SAM lacks text-prompt capabilities, it provides superior accuracy. By integrating CLIPSeg’s text-based identification with SAM’s precision, we achieved more accurate and flexible segmentation.\n\n\n\nCLIPSeg combines a vision transformer with text prompts for zero-shot segmentation, enabling flexible and precise object identification by leveraging large-scale image-text pairs.\n\n\n\n\nPixel Ratio Conversion: Translating Pixels to Real-World Dimensions\nTo convert bounding box measurements into real-world dimensions, we used pixel ratio conversion equations. By establishing a pixel-to-actual-dimension ratio using the known diameter of a sticker, we could accurately translate the height and width of the components into centimeters, ensuring precise dimensional predictions crucial for quality control in the assembly process."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#implementation",
    "href": "blogs/robotic_arms_vision_recognition/index.html#implementation",
    "title": "Robotic Arm Vision Recognition",
    "section": "Implementation",
    "text": "Implementation\nThe implementation involved several key steps:\nData Collection and Processing: We gathered a diverse dataset of images containing various objects to train and validate the models. The dataset included images with different lighting conditions, backgrounds, and object orientations. Labelling was done using Segments.ai.\nModel Training and Fine-Tuning: Mask R-CNN, SAM, and ClipSeg were trained and fine-tuned on the collected dataset. Each model was optimized to perform specific tasks—Mask R-CNN for object detection and segmentation, SAM for versatile segmentation, and ClipSeg for semantic understanding.\nMeasurement: Measure bottle components dimension using mathematical transformations which is essentially pixel-ratio-dimensions.\nEvaluation: Assess model performance using metrics such as Intersection over Union (IoU) and recall. Evaluate dimension measurement performance using metrics such as percentage error.\n\n\n\nImages with diverse conditions were labeled using Segments.ai and processed through two approaches: training and fine-tuning Mask R-CNN; and applying zero-shot segmentation with ClipSeg + SAM. Bottle component measurements were derived from pixel-to-dimension ratios, and model performance was evaluated using IoU, recall, and percentage error metrics."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#a-comparative-analysis-mrcnn-vs-clipseg-sam",
    "href": "blogs/robotic_arms_vision_recognition/index.html#a-comparative-analysis-mrcnn-vs-clipseg-sam",
    "title": "Robotic Arm Vision Recognition",
    "section": "A Comparative Analysis: MRCNN vs CLIPSeg + SAM",
    "text": "A Comparative Analysis: MRCNN vs CLIPSeg + SAM\nWe used the Mean Average Percentage Error (MAPE) to evaluate the performance of MRCNN versus the CLIPSeg + SAM combination. CLIPSeg + SAM consistently outperformed MRCNN in predicting bottle dimensions, with lower percentage errors in height (2.35% vs. 9.96%) and width (3.22% vs. 12.2%). Despite some variability, CLIPSeg + SAM demonstrated superior accuracy overall. Improving image quality is expected to further enhance this combination’s precision, leading to even greater accuracy in future applications.\n\n\n\n\n\n\nSegmentation results from the fine-tuned Mask R-CNN model.\n\n\n\n\n\n\n\nCLIP output (Left) to be cross-referenced with SAM’s outputs (small pictures to its right). Numbers below are the IoU scores"
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#challenges-and-learnings",
    "href": "blogs/robotic_arms_vision_recognition/index.html#challenges-and-learnings",
    "title": "Robotic Arm Vision Recognition",
    "section": "Challenges and Learnings",
    "text": "Challenges and Learnings\nThe dataset size significantly impacted MRCNN’s performance. Expanding the dataset would likely improve the model’s generalization ability. Additionally, using higher-quality images with clear component boundaries could enhance segmentation accuracy. Incorporating a cropping step in the pre-processing pipeline might help focus the model on relevant areas, potentially improving performance. However, these enhancements were beyond the current project scope due to time and resource constraints."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#future-work-and-opportunities-for-improvement",
    "href": "blogs/robotic_arms_vision_recognition/index.html#future-work-and-opportunities-for-improvement",
    "title": "Robotic Arm Vision Recognition",
    "section": "Future Work and Opportunities for Improvement",
    "text": "Future Work and Opportunities for Improvement\nLooking ahead, there are several opportunities to extend this work. Expanding the dataset, incorporating new models, or experimenting with different architectures could yield even better results. Furthermore, the integration of additional features like real-time processing or multi-view analysis could enhance the system’s capability and applicability across various industries."
  },
  {
    "objectID": "blogs/robotic_arms_vision_recognition/index.html#conclusion",
    "href": "blogs/robotic_arms_vision_recognition/index.html#conclusion",
    "title": "Robotic Arm Vision Recognition",
    "section": "Conclusion",
    "text": "Conclusion\nThe “Robotic Arm Vision Recognition” project showcased the transformative potential of advanced computer vision techniques in enhancing robotic systems. By integrating Mask R-CNN, Segment Anything Model (SAM), and CLIPSeg, we achieved significant improvements in object detection, segmentation, and classification. This project underscores the importance of leveraging state-of-the-art technologies to tackle complex challenges in computer vision and robotics.\nAs technology continues to advance, integrating such innovative models will undoubtedly lead to even more capable and intelligent robotic systems. The insights gained from this project provide a solid foundation for future developments in robotic vision technology."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Autopredictor: Python Package\n\n\n11 min\n\n\nBuilding a Python package: Utilizing cookiecutter template for its simplicity and ease of use\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiabetes Classification Model: Virtual Environment through Docker\n\n\n9 min\n\n\nBuilding a Diabetes Classification Model: Leveraging Docker for Efficient Environment Management.\n\n\n\nFeb 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobotic Arm Vision Recognition\n\n\n7 min\n\n\nAn end-to-end data pipeline for instance segmentation and classification\n\n\n\nAug 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWaste Classification with AI\n\n\n7 min\n\n\nBuilding an End-to-End pipeline inspired by YVR\n\n\n\nSep 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\neCommerce Analytics using AWS\n\n\n8 min\n\n\nSimulating the logs of user purchases, product views, cart history, and the user’s journey to…\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html",
    "href": "blogs/ecommerce_analytics/index.html",
    "title": "eCommerce Analytics using AWS",
    "section": "",
    "text": "In this AWS Big Data project, I use an eCommerce dataset to simulate the logs of user purchases, product views, cart history, and the user’s journey to build batch and real-time pipelines."
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html#objectives",
    "href": "blogs/ecommerce_analytics/index.html#objectives",
    "title": "eCommerce Analytics using AWS",
    "section": "Objectives",
    "text": "Objectives\n\nunderstanding ETL on Big Data\nIntroduction to Staging and Data Lake\nCreating IAM Roles and Policies\nUnderstanding the Dataset\nSetting up AWS CLI\nUnderstanding Data Streams and Amazon Kinesis\nUnderstanding Apache Flink\nCreating a Kinesis Data Analytics Application\nUsing Glue and Athena to define Partition Key\nUnderstanding Lambda Functions\nCreating Lambda function for DynamoDB and SNS\nUnderstanding DynamoDB Data Modelling\nIntegrating Lambda and Kinesis\nPerforming ETL for Parquet format using Glue DataBrew and Spark\nCreating QuickSight Dashboards"
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html#overview",
    "href": "blogs/ecommerce_analytics/index.html#overview",
    "title": "eCommerce Analytics using AWS",
    "section": "Overview",
    "text": "Overview\n\nBusiness Overview\nEcommerce analytics is the process of collecting data from all of the sources affect a certain shop. Analysts can then utilize this information to deduce changes in customer behavior and online shopping patterns. Ecommerce analytics spans the whole customer journey, from discovery through acquisition, conversion, and eventually rertention and support.\nIn this project, we will use an eCommerce Dataset to simulate the logs of user purchase, product views, cart history, and the user’s journey on the online platform to create two analytical pipelines, Batch and Real-time. The Batch processing will involve data ingestion, Lake House architecture, processing, visualization using Amazon Kinesis, Glue, S3, and QuickSight to daw insights regarding the following: - Unique visitors per day - During a certain time, the users add products to their carts but don’t buy them - Top categories per hour or weekday (i.e. to promote discounts based on trends) - To know which brands need more marketing\nThe Real-time channel involves detecting Distributed denial of Services(DDoS) and Bot attacks using AWS Lambda, DynamoDB, CloudWatch, and AWS SNS.\n\n\nData Pipeline\nA data pipeline is a technique for transferring data from one system to another. The data may or may not be updated, and it maybe handled in real-time (or streaming) rather than in batches. The data pipeline encompasses everything from harvesting or acquiring data using various methods to storing raw data, cleaning, validating, and transforming data into a query-worthy format, displaying KPIs, and managing the above process\n\n\nDataset Description\nThis dataset contains user behavioral information from a large multi-category online store along with fields such as event_time, event_type, product_id, price, user_id. Each row in the file represents one of the following event types: - View - Cart - Removed from Cart - Purchase"
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html#aws-introduction",
    "href": "blogs/ecommerce_analytics/index.html#aws-introduction",
    "title": "eCommerce Analytics using AWS",
    "section": "AWS Introduction",
    "text": "AWS Introduction\n\nAmazon S3: an object storage service that provides manufacturing scalability, data availability, security and performance. Users may save and retrieve any quantity of data using Amazon S3 at any time and from any location.\nAWS Glue: A serverless data integration service makes it easy to discover, prepare, adn combine data for analytics, machine learning, and application development. It runs Spark/Python code without managing Infrastructure at a nominal cost. You pay only during the run time of the job. ALso, you pay storage costs fo Data Catalog objects. Tables may be added to the AWS Glue Data Catalog using a Crawler. The majorrity of the AWS Glue users employ this strategy. In a single un, a crawler can crawl numerous data repositories. The crawler adds or modified one or more tables in your Data Catalog after it’s finished.\nAWS Athena: An interactive query service for S3 in which there is no need to load data, and it stays in S3. It is serverless and supports many data format, e.g., CSV, JSON, ORC, Parquet, AVRO.\nApache Flink: A scalable data analytics platform and distributed processing engine. Flink may be used to handle massive data streams and give real-time analytical insights about the processed data to your streaming application. FLink is built to work in a variety of cluster setups, with in-memory calculations of any size. For distributed computations over data streams, Flink also offer communication, fault tolerance, and data distribution. Flink applications use unbounded or bounded data sets to process streams of events. Unbounded streams have no fixed termination and are handled indefinitely. Bounded streams have a defined beginning and endpoint and may be handles in batches.\nAmazon Kinesis Data Stream: A real-time data collection and prcoessing service from Amazon. Kinesis Data Stream apps are data-processing application that may be created. Data Firehose is part of the Kinesis streaming data platform, which also includes Kinesis Data STrerams, Kinesis Video Sterams, and Amazon Kinesis Data Analytics. When using Kinesis Data Firehose, the user does not need to develop apps or manage resources. Configure the data producers to send data to Kinesis Data Firehose and the data will be automatically transferred to the specified destination. Kinesis Data Firehose may also be used to transform data before delivering it.\nQuickSight: A scalable, serverless embeddable, machine learning-powered business intelligence (BI) service built for the cloud. It is the first BI service to offer pay-per-session pricing, where you only pay when your users access their dashboards or reports, making it cost-effectively for large-scale deployments. It can connect to various sources like Redshift, S3, Dynamo, RDS, files like JSON text, CSV, TSV, Jira, Salesforce, and on-premises oracale SQL-server\nAWS Glue DataBrew: A visual data preparation tool that allow users to clean and normalize data without writing any code. When compared to custom-created data preparation, DataBrew helps minimize the time it takes to prepare data for analytics and machine learning. Many pre-built transformations are available to automate data preparation activities such as screening anomalies, transforming data to standard formats, and rectifying erroneous values.\nAmazon DynamoDB: A fully managed key-value NoSQL databse service that delivers quick and predictable performance while also allowing for seamless scaling. DyanamoDB relieves developers of the administrative responsibilities associated with running and growing a distributed database. DynamoDB allows you to design databse tables that can store and retrieve any quantity of data while also serving any degree of request volume. You may increase or decrease the throughput capacity of your tables without experiencing downtime or performance reduction."
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html#use-case-simulation",
    "href": "blogs/ecommerce_analytics/index.html#use-case-simulation",
    "title": "eCommerce Analytics using AWS",
    "section": "Use case simulation",
    "text": "Use case simulation\n\nBrand’s popularity: to know which brands I need to invest more marketing\nUser’s journey: why after 11pm, the users add products to their carts, but dont buy them in the end\nKnow our customers: unique visitors per day; top categories per hour or weekday (i.e. to promote discounts basedo on trends)\nsystems are normally designed on constant patterns which is not scalable. Latency increase with peaks and latency in data leads to useless data\nTimely decisions requiers new data in minutes as data loses value quickly over time"
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html#success-criteria",
    "href": "blogs/ecommerce_analytics/index.html#success-criteria",
    "title": "eCommerce Analytics using AWS",
    "section": "Success Criteria",
    "text": "Success Criteria\n\nData ingestion: simulate a NRT stream and scheduled bulk/batch\nLake House and Stream design: Design and build a new Lake house architecture, including Batch and NRT laters\nAWS CLoud\nReal-Time decisions: trigger mechanism based on stream\nScalabilitty: the data architecture should scale efficiently\nReporting: Dashboard withh relevant Business Data\n\n\nHow to stream data by simulating real data producer: Amazon Kinesis and Python\n\ndetermine which products are being sold in near real-time\n\nHow to process that incoming stream and create triggering mechanisms: Python, Apache Flink, Amazon S3, AWS SNS, AWS Lambda\n\nif same user_id views &gt;10 products/sec, send immediately an alert -&gt; possible BOT/DDos Attack\n\nHow to process hourly data in batch layer: AWS DataSync tasks\nBusiness Intelligence layher: Amazon QuickSIght for BI reporting\n\nDataset from kaggle: https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store\nOption 1: download locally and copy to AWS Option 2: start with AWS, you can download it directly Option 3: use smaller dataset , confirm the data model is the same\nOption 1: Set up AWS CLI; Create S3 bucket; Copy dataset to AWS\ncreate a s3 bucket\naws Configure\naws s3 ls # list all s3 bucket\naws s3 ls s3://ecommerce-raw-euwest-14317-dev/ #list objects inside the bucket\ncd downloads\naws s3 cp 2019-Nov.csv.zip s3://ecommerce-raw-euwest-14317-dev/ # copy the zip file in download folder to the s3 bucket\nNow, I have a csv in a S3 bucket and I will want a python code to read the CSV to simulate streaming. I want to deploy a Kinesis stream.\n\nBatch processing\nData scope: queries or processing overl all or most of the data in the dataset Data size: Lage batches of data Performance: Latencies in min to hours Analysis: Complex analytics\n\n\nStream processing\nData scope: Queries or processing over data within a rolling time window, or on just the most recent data rrecord Data size: Individual record or micro batches consisting of a few records Performance: Requires latency in the order of seconds or milliseconds Analysis: Simple response functions, aggregates, and rolling metrics"
  },
  {
    "objectID": "blogs/ecommerce_analytics/index.html#amazon-kinesis",
    "href": "blogs/ecommerce_analytics/index.html#amazon-kinesis",
    "title": "eCommerce Analytics using AWS",
    "section": "Amazon Kinesis",
    "text": "Amazon Kinesis\n\neasily collect, process, and analyze data streams in real time\n\n\nData Streams: Collect and store data stream for analytics\nData Firehose: Load data streams onto AWS data stores\nData Analytics: Analyze data streams with Amazon Kinesis Data Analytics Studio/ Apache Flink\nVideo Streams: Collect and store video streams for analytics\n\nso now we have a python application pushing our CSV data into data stream 1 - I have an input which needs to be captured and sent data to Kinesis Data streams –&gt; then data stream is ingested and stored for processing in Kinesis Data Analytics/ Firehose / EC2/ Lambda –&gt; output: analyze streaming data using your favourite BI tools\n\nCrerating data stream\n\ngo tto"
  }
]